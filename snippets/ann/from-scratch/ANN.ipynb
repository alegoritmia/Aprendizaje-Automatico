{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-toolbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plot import plot_decision_boundary\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-rogers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def z(input, Theta):\n",
    "    # print('Theta is {} in transpose {}, input is {}'.format(Theta.shape, Theta.T.shape, input.shape))\n",
    "    # we need to get an n x m array, where n is the neurons in this layer and m is the number of activations we used\n",
    "    return Theta @ input\n",
    "\n",
    "def activation(z):\n",
    "    return sigmoid(z)\n",
    "\n",
    "def initialize_weights(X, num_classes, hidden):\n",
    "    weights = []\n",
    "    out_neurons = num_classes \n",
    "    n = X.shape[0]\n",
    "    \n",
    "    h_layers = len(hidden)\n",
    "    # print('Hidden layers will be {}'.format(h_layers))\n",
    "    s_j = n\n",
    "    # [2]\n",
    "    for j in range(0,h_layers):\n",
    "        s_jplus1 = hidden[j]\n",
    "        cols = s_j + 1\n",
    "        # print('Theta {} will be {}x{}'.format(j+1, s_jplus1, cols))\n",
    "        # weights_layer = np.zeros((s_jplus1, cols))\n",
    "        weights_layer = np.random.rand(s_jplus1, cols) # between 0,1\n",
    "        # weights_layer = np.random.uniform(low=-10, high=10, size=(s_jplus1, cols)) # between a range\n",
    "        weights.append(weights_layer)\n",
    "        s_j = s_jplus1\n",
    "    \n",
    "    weights.append(np.random.rand(out_neurons, s_j + 1)) # between 0-1\n",
    "    # weights.append(np.random.uniform(low=-10, high=10, size=(out_neurons, s_j + 1))) # between a range\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def print_matrix(matrix):\n",
    "    for e in matrix:\n",
    "        print('shape: {} {}'.format(e.shape, e))\n",
    "\n",
    "def initialize_activations(X, output_neurons, hidden):\n",
    "    # initialize activations, they are\n",
    "    # input layer: (n+1) x m, \n",
    "    # hidden: (each + 1) x m,\n",
    "    # output: output_neurons x m\n",
    "    # So we can represent them with a list of length 2 + len(hidden)\n",
    "    m = X.shape[1] # dimension 0 = features, dimension 1 = # examples\n",
    "    activations = []\n",
    "    \n",
    "    # input layer\n",
    "    a_1 = X\n",
    "    biases = np.ones(m)\n",
    "    a_1 = np.vstack((biases, a_1))\n",
    "    activations.append(a_1)\n",
    "\n",
    "    for i in range(len(hidden)):\n",
    "        a_i = np.zeros((hidden[i] + 1 , m))\n",
    "        a_i[0,:] = 1.0\n",
    "        # a_i[0,0] = 1.0 # TODO: Initialize in 1 all activations for bias neurons. Previous line does it\n",
    "        activations.append(a_i)\n",
    "\n",
    "    # output layer\n",
    "    activations.append(np.zeros((output_neurons, m)))\n",
    "    return activations\n",
    "\n",
    "def forward(X, hidden, activations, weights):\n",
    "    # Hidden layers\n",
    "    for i in range(0, len(hidden)):\n",
    "        a_i = activations[i]\n",
    "        # print('In Layer {}'.format(i+1))\n",
    "        z_next = z(a_i, theta[i])\n",
    "        # print('z_i are {}'.format(z_next.T))\n",
    "        a_next = activation(z_next)\n",
    "        # this line would fail for output layer, that's why process that separatedly \n",
    "        activations[i+1][1:] = a_next \n",
    "        # print('activations {} are {}'.format(i+1, a_next))\n",
    "\n",
    "    # output layer (i+2 is the output layer)\n",
    "    a_i = activations[i+1]\n",
    "    z_next = z(a_i, theta[i+1])\n",
    "    a_next = activation(z_next)\n",
    "    activations[i+2] = a_next\n",
    "    # print('output is {}'.format(a_next.T))\n",
    "\n",
    "def backprop(y, hidden, activations, theta, alpha, reg):\n",
    "    m = y.shape[1]\n",
    "    delta = []\n",
    "    # Calculating local gradients\n",
    "    # Output layer\n",
    "    y_pred = activations[-1]\n",
    "    delta_i = y_pred - y\n",
    "    # print('Error is {}'.format(delta_i.T))\n",
    "    delta.append(delta_i)\n",
    "\n",
    "    start = len(activations) - 1\n",
    "\n",
    "    # Hidden layers\n",
    "    for i in range(start, 1, -1): # we don't calculate errors for input layer\n",
    "        # print('In layer {}'.format(i))\n",
    "        theta_prev = theta[i-1][:,1:] # this is ignoring bias\n",
    "        tmp = theta_prev.T @ delta_i\n",
    "        # print('tmp is {}'.format(tmp.T))\n",
    "        delta_i = tmp * ( activations[i-1][1:] * (1 - activations[i-1][1:])) # this is ignoring bias\n",
    "        # print('Error is {}'.format(delta_i.T))\n",
    "        delta.append(delta_i)\n",
    "    \n",
    "    # delta list holds the values\n",
    "    # delta length is all but first layer (layers-1)\n",
    "    # we could add an extra column for input layer to have the same\n",
    "    # indexes as in the slides.\n",
    "    delta.reverse()\n",
    "    # print(delta)\n",
    "\n",
    "    # Calculating Deltas\n",
    "    num_clases = theta[-1].shape[0]\n",
    "    Delta = initialize_weights(X, num_clases, hidden) # The Delta has the same dimensions as the weight matrix (we will ignore bias though)\n",
    "    # This is for weights, not for neurons, that is why we reach layer 1\n",
    "    start = len(delta) - 1\n",
    "    for i in range(start, -1, -1):\n",
    "        # print('Now in layer {}'.format(i+1))\n",
    "        activations_this_layer = activations[i][1:,:] # this is ignoring bias\n",
    "        delta_next_layer = delta[i]\n",
    "        activations_times_delta = activations_this_layer@delta_next_layer.T\n",
    "        Delta[i][:,1:] = activations_times_delta.T \n",
    "    \n",
    "    D = [ x/m for x in Delta]\n",
    "\n",
    "    for i in range(len(D)):\n",
    "        d = D[i]\n",
    "        t = theta[i]\n",
    "        d[:,1:] += reg * t[:,1:]\n",
    "    \n",
    "    # update rule\n",
    "    for i in range(len(theta)):\n",
    "        t = theta[i]\n",
    "        d = D[i]\n",
    "\n",
    "        t[:, 1:] = t[:, 1:] - (alpha * d[:, 1:])\n",
    "\n",
    "def predict(X, theta):\n",
    "    a_i = X\n",
    "    # all theta layers\n",
    "    for i in range(len(theta)):\n",
    "        # print('In layer {}'.format(i+1))\n",
    "        biases = np.ones(a_i.shape[1])\n",
    "        a_i = np.vstack((biases, a_i))\n",
    "\n",
    "        z_next = z(a_i, theta[i])\n",
    "        a_i = activation(z_next)\n",
    "\n",
    "    return a_i\n",
    "\n",
    "def get_one_hot(targets, nb_classes):\n",
    "    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "    return res.T\n",
    "\n",
    "def cost_function(activations, y, theta, reg_factor):\n",
    "    m = y.shape[1]\n",
    "    y_pred = activations[-1]\n",
    "\n",
    "    left_part = y * np.log(y_pred)\n",
    "    right_part = (1 - y)*np.log(1 - y_pred)\n",
    "    sum = np.sum(left_part + right_part)\n",
    "    sum = -sum/m\n",
    "\n",
    "    # Here, T[:, 1: ] means everything but the weight from the first row, \n",
    "    # that is the weight from bias\n",
    "    reg_part_per_layer = [ np.sum(T[:, 1: ]**2) for T in theta]\n",
    "    reg_part = np.sum(reg_part_per_layer) * reg_factor / (2*m)\n",
    "    return sum + reg_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-provider",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print('Starting...')\n",
    "#     data = pd.read_csv('blobs.csv')\n",
    "    data = pd.read_csv('circles.csv')\n",
    "#     data = pd.read_csv('moons.csv')\n",
    "#     data = pd.read_csv('xor.csv')\n",
    "    X = data.iloc[:,:-1].to_numpy().T   # all but last column of labels\n",
    "    y = data.iloc[:,-1].to_numpy()      # the last col is class\n",
    "    y = y.reshape(-1,1)  # to get an mx1 array and not (m,)\n",
    "    unique_classes = len(np.unique(y))\n",
    "    y = get_one_hot(y, unique_classes) # 1 -> [0, 1, 0], 2 -> [0, 0, 1], 0->[1, 0, 0]\n",
    "    \n",
    "\n",
    "    # # This part should go to a fit function >>\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X.T, y.T, test_size = 0.20)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = X_train.T, X_test.T, y_train.T, y_test.T\n",
    "    # For xor dataset\n",
    "#     hidden = [6,6]\n",
    "#     reg_factor = 0.0\n",
    "#     alpha=0.5\n",
    "#     epochs = 100000\n",
    "\n",
    "    # # For blobs dataset\n",
    "#     hidden = [2,5]\n",
    "#     reg_factor = 0.0\n",
    "#     alpha=0.05\n",
    "#     epochs = 5000 # 50000\n",
    "\n",
    "    # # For circles dataset\n",
    "    hidden = [9, 9, 9]\n",
    "    reg_factor = 0\n",
    "    alpha = 0.05\n",
    "    epochs = 500000\n",
    "\n",
    "    # # For moons dataset\n",
    "    # hidden = [4,4,4]\n",
    "    # reg_factor = 0.0\n",
    "    # alpha=0.1\n",
    "    # epochs = 500000\n",
    "\n",
    "    theta = initialize_weights(X_train, unique_classes, hidden)\n",
    "    activations = initialize_activations(X_train, unique_classes, hidden)\n",
    "     # For xor dataset\n",
    "#     hidden = [6,6]\n",
    "\n",
    "    # # For blobs dataset\n",
    "#     hidden = [2,5]\n",
    "\n",
    "    # # For circles dataset\n",
    "#     hidden = [9, 9, 7]\n",
    "\n",
    "    # # For moons dataset\n",
    "#     theta[0][0,0] = 0.35\n",
    "#     theta[0][1,0] = 0.15\n",
    "#     theta[0][2,0] = 0.20\n",
    "#     theta[0][3] = 0.1\n",
    "#     theta[0][4] = 0.1\n",
    "#     theta[0][5] = 0.1\n",
    "#     theta[0][6] = 0.5\n",
    "#     theta[0][7] = 0.5\n",
    "#     theta[0][8] = 0.5\n",
    "    \n",
    "#     theta[1][0,0] = 1\n",
    "#     theta[1][1,0] = 0.5\n",
    "#     theta[1][2,0] = 0.1\n",
    "#     theta[1][3] = 0.1\n",
    "#     theta[1][4] = 0.1\n",
    "#     theta[1][5] = 0.1\n",
    "#     theta[1][6] = 0.1\n",
    "#     theta[1][7] = 0.1\n",
    "#     theta[1][8] = 0.1\n",
    "    \n",
    "#     theta[2][0,0] = 1\n",
    "#     theta[2][1,0] = 0.1\n",
    "#     theta[2][2,0] = 0.1\n",
    "#     theta[2][3] = 0.1\n",
    "#     theta[2][4] = 0.1\n",
    "#     theta[2][5] = 0.1\n",
    "#     theta[2][6] = 0.5\n",
    "#     theta[2][7] = 0.5\n",
    "#     theta[2][8] = 0.5\n",
    "    \n",
    "\n",
    "    costs = []\n",
    "    \n",
    "    for e in range(epochs):\n",
    "         # print('Epoch {} '.format(e))\n",
    "         forward(X_train, hidden, activations, theta)\n",
    "         cost = cost_function(activations, y_train, theta, reg_factor)\n",
    "         costs.append(cost)\n",
    "         backprop(y_train, hidden, activations, theta, alpha, reg_factor)\n",
    "    # << This part should go to a fit function\n",
    "    \n",
    "    plot1 = plt.figure(1)\n",
    "    plt.plot(range(len(costs)), costs)\n",
    "\n",
    "    plot_decision_boundary(X.T, y.T, predict, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-bottle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
